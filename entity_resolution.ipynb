{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sklearn as skl\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import ngrams\n",
    "import random\n",
    "import string\n",
    "import Levenshtein as lv\n",
    "import nltk\n",
    "import textdistance\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import deepmatcher as dm\n",
    "from torchtext import vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creazione delle feature dei trigrammi\n",
    "def create_features(titles):\n",
    "    features_vec_products = []\n",
    "    \n",
    "    for title_tkn in titles:\n",
    "        for i, gram in enumerate(title_tkn):\n",
    "            d = {}\n",
    "            features_vec = []\n",
    "            #distanza dall'inizio e dalla fine (0, 1)\n",
    "            idx = title_tkn.index(gram)\n",
    "            features_vec.append(idx)\n",
    "            features_vec.append(len(title_tkn) - idx)\n",
    "\n",
    "            # almeno una stringa è scritta in maiuscolo (2)\n",
    "            flag_maiusc = 0\n",
    "            for word in gram.split():\n",
    "                if word.isupper():\n",
    "                    features_vec.append(1)\n",
    "                    flag_maiusc = 1\n",
    "                    break\n",
    "            if flag_maiusc == 0:\n",
    "                features_vec.append(0)\n",
    "                \n",
    "            #tutte le stringe sono scritte in maiuscolo (3)\n",
    "            flag_all_maiusc = 1\n",
    "            for word in gram.split():\n",
    "                if not word.isupper():\n",
    "                    flag_all_maiusc = 0\n",
    "                    features_vec.append(0)\n",
    "                    break\n",
    "            if flag_all_maiusc == 1:\n",
    "                features_vec.append(1)\n",
    "\n",
    "            #almeno una stringa contiene numeri (4)\n",
    "            flag_number = 0\n",
    "            for word in gram.split():\n",
    "                if re.search(regex_hasNum, word):\n",
    "                    features_vec.append(1)\n",
    "                    flag_number = 1\n",
    "                    break\n",
    "            if flag_number == 0:\n",
    "                features_vec.append(0)\n",
    "                \n",
    "            #almeno una stringa contiene solo numeri (5)\n",
    "            flag_only_numbers = 0\n",
    "            for word in gram.split():\n",
    "                if word.isdecimal():\n",
    "                    features_vec.append(1)\n",
    "                    flag_only_numbers = 1\n",
    "                    break\n",
    "            if flag_only_numbers == 0:\n",
    "                features_vec.append(0)\n",
    "            \n",
    "            #contiene nome del sito (6)\n",
    "            flag_site = 0\n",
    "            for word in gram.split():\n",
    "                if word.lower() in '\\t'.join(sites):\n",
    "                    flag_site = 1\n",
    "                    features_vec.append(1)\n",
    "                    break\n",
    "            if flag_site == 0:\n",
    "                features_vec.append(0)\n",
    "\n",
    "            #almeno una stringa è una specifica (7)\n",
    "            flag_spec = 0\n",
    "            for word in gram.split():\n",
    "                for regex in specs_re:\n",
    "                    if re.search(regex, word):\n",
    "                        flag_spec = 1\n",
    "                        break\n",
    "                if flag_spec == 1:\n",
    "                    features_vec.append(1)\n",
    "                    break\n",
    "            if flag_spec == 0:\n",
    "                features_vec.append(0)\n",
    "\n",
    "            #almeno una stringa è brand (8)\n",
    "            flag_brand = 0\n",
    "            for word in gram.split():\n",
    "                if word.lower() in '\\t'.join(brands):\n",
    "                    flag_brand = 1\n",
    "                    features_vec.append(1)\n",
    "                    break\n",
    "            if flag_brand == 0:\n",
    "                features_vec.append(0)\n",
    "                \n",
    "            #prima stringa è brand e c'è un codice (9)\n",
    "            flag_brand = 0\n",
    "            if gram.split()[0].lower() in '\\t'.join(brands) and (re.search(regex_code, gram.split()[1]) or re.search(regex_code, gram.split()[2])):\n",
    "                features_vec.append(1)\n",
    "            else:\n",
    "                features_vec.append(0)\n",
    "\n",
    "            #almeno una str codice (10)\n",
    "            flag_code = 0\n",
    "            for word in gram.split():\n",
    "                if re.search(regex_code, word):\n",
    "                    flag_code = 1\n",
    "                    break\n",
    "            features_vec.append(flag_code)\n",
    "            \n",
    "            #tutte str codice (11)\n",
    "            flag_all_code = 1\n",
    "            for word in gram.split():\n",
    "                if not re.search(regex_code, word):\n",
    "                    flag_all_code = 0\n",
    "                    break\n",
    "            features_vec.append(flag_all_code)\n",
    "\n",
    "            d['gram'] = gram\n",
    "            d['vector'] = features_vec\n",
    "\n",
    "            features_vec_products.append(d)\n",
    "    return features_vec_products\n",
    "\n",
    "# estrae predizioni errate\n",
    "def wrong_pred(true, predicted, idx):\n",
    "    wrong_indexes = []\n",
    "    for i in range(len(idx)):#df['label'].to_list()):\n",
    "        if true[i] != predicted[i]:\n",
    "            wrong_indexes.append(idx[i])\n",
    "\n",
    "    return wrong_indexes\n",
    "\n",
    "# creazione delle combinazioni dei token per i trigrammi notevoli\n",
    "def create_string_combination(string, df, idx, l_or_r):\n",
    "    res = []\n",
    "    tok = string.split(' ')\n",
    "    length = len(tok)\n",
    "    for i in range(0, length):\n",
    "        #print(df['{}_brand'.format(l_or_r)][idx])\n",
    "        if isinstance(df['{}_brand'.format(l_or_r)][idx], str) and (df['{}_brand'.format(l_or_r)][idx] not in tok[i].lower()):\n",
    "            if re.search(regex_code, tok[i]) and not [True for rgx in specs_re if re.search(rgx, tok[i])]:\n",
    "                res.append(df['{}_brand'.format(l_or_r)][idx]+tok[i])\n",
    "        if not tok[i].isdigit():\n",
    "            res.append(tok[i])\n",
    "        if i < length - 1:\n",
    "            if not tok[i].isdigit() and not tok[i + 1].isdigit():\n",
    "                res.append(tok[i] + tok[i+1])\n",
    "    res.append(string.replace(' ', ''))\n",
    "    return list(set(res))\n",
    "\n",
    "#creazione dei trigrammi\n",
    "def create_trigrams(df, l_or_r, limit): #limit=len(df)\n",
    "    d = {'{}_codes'.format(l_or_r): []}\n",
    "    to_ngrams = []\n",
    "    for i in range(0, limit):\n",
    "        if (i >= limit):\n",
    "            break\n",
    "        no_punc = (df['{}_page_title'.format(l_or_r)][i].replace('\"', '').replace(\"Null\", \"\").translate(str.maketrans('', '', string.punctuation)))\n",
    "        for i in range(0,len(no_punc)):\n",
    "            if (i + 1) < len(no_punc) and re.search(regex_code, no_punc[i]) and re.search(regex_code, no_punc[i + 1]):\n",
    "                to_ngrams.append(no_punc[i] + no_punc[i + 1])\n",
    "                i += 1\n",
    "            else:\n",
    "                to_ngrams.append(no_punc[i])\n",
    "        tokenized = ngrams(to_ngrams.split(), 3)\n",
    "        d['{}_codes'.format(l_or_r)].append([create_string_combination(' '.join(grams)) for grams in tokenized][0])\n",
    "    return d\n",
    "\n",
    "# creazione del dataset finale\n",
    "def prepare_final_data(df, tok_titles, l_or_r, clf, limit=None):\n",
    "    if limit == None:\n",
    "        limit = len(df)\n",
    "    final_list = []\n",
    "    for idx, item in enumerate(tok_titles):\n",
    "        if idx >= limit:\n",
    "            break\n",
    "        print(\"{}/{}\\r\".format(idx, limit if limit else len(tok_titles)), flush=True, end=\"\")\n",
    "        page_id = df[\"{}_spec_id\".format(l_or_r)][idx]\n",
    "        brand = df[\"{}_brand\".format(l_or_r)][idx]\n",
    "        features = create_features([item])\n",
    "        df_tkn = pd.DataFrame(features)\n",
    "        l = []\n",
    "        codes = []\n",
    "        for index, word in df_tkn.iterrows():\n",
    "            d = {}\n",
    "            vec = ((word['vector']))\n",
    "            for i in range(0, 12):\n",
    "                d['feature_' + str(i)] = vec[i]\n",
    "            l.append(d)\n",
    "            #df usato per svm\n",
    "        df_features = pd.DataFrame(l)\n",
    "        pred = clf.predict(df_features)\n",
    "        for i, tag in enumerate(pred):\n",
    "            if tag == 1:\n",
    "                codes= codes+create_string_combination(item[i], df, idx, l_or_r)\n",
    "        final_list.append({\"{}_spec_id\".format(l_or_r): page_id, \"{}_codes\".format(l_or_r): codes, \"{}_brand\".format(l_or_r): brand})\n",
    "    return final_list\n",
    "\n",
    "# calcolo della similarità tra due offerte per valutare il matching\n",
    "def compute_score(ev_df, distance = 'jaccard', th=0.9):\n",
    "    indexes = ev_df[0]\n",
    "    left_codes = ev_df[2]\n",
    "    left_brand = ev_df[3]\n",
    "    right_codes = ev_df[5]\n",
    "    right_brand = ev_df[6]\n",
    "    \n",
    "    predicted_nb = []\n",
    "    idx_to_dm = []\n",
    "    score = []\n",
    "    comp = []\n",
    "    empty_idx_list = []\n",
    "    \n",
    "    for i in range(0, len(indexes)):\n",
    "        print(i, end='\\r', flush=True)\n",
    "        if not isinstance(left_brand[i], float) and not isinstance(right_brand[i], float) and (left_brand[i].upper() != right_brand[i].upper()):\n",
    "            predicted_nb.append(0)\n",
    "            score.append(0)\n",
    "            comp.append('EXLUDED BY BRANDS: {} ------ {}'.format(left_brand[i], right_brand[i]))\n",
    "            continue\n",
    "        if not left_codes[i] or not right_codes[i]:\n",
    "            predicted_nb.append(0)\n",
    "            idx_to_dm.append(indexes[i])\n",
    "            score.append(0)\n",
    "            comp.append('TO_DM')\n",
    "            empty_idx_list.append(i)\n",
    "            continue\n",
    "        max_score = 0\n",
    "        match = 0\n",
    "        comparisons = ''\n",
    "        left_target = None\n",
    "        right_target = None\n",
    "        for item in left_codes[i]:\n",
    "            for item_r in right_codes[i]:\n",
    "                if distance == 'lev':\n",
    "                    match_score = lv.ratio(item.upper(), item_r.upper())\n",
    "                elif distance == 'jaccard':\n",
    "                    #match_score = 1 - nltk.jaccard_distance(set(item), set(item_r))\n",
    "                    match_score = textdistance.jaccard.similarity(item.upper(), item_r.upper())\n",
    "                elif distance == 'jw':\n",
    "                    match_score = textdistance.jaro_winkler.similarity(item.upper(), item_r.upper())\n",
    "                elif distance == 'cosine':\n",
    "                    match_score = textdistance.cosine.similarity(item.upper(), item_r.upper())\n",
    "                else:\n",
    "                    return []\n",
    "                if match_score > max_score and re.search(regex_code, item) and re.search(regex_code, item_r) and item.lower() not in '\\t'.join(sites) and item_r.lower() not in '\\t'.join(sites) and len([True for reg in specs_re if re.search(reg, item)]) == 0 and len([True for reg in specs_re if re.search(reg, item_r)]) == 0:\n",
    "                    max_score = match_score\n",
    "                    left_target = item\n",
    "                    right_target = item_r\n",
    "                    comparisons = left_target + '---> ' + right_target + ' -----SCORE: ' + str(max_score) + ' ------IDX' + str(indexes[i])\n",
    "        if max_score > th:\n",
    "            match = 1\n",
    "        predicted_nb.append(match)\n",
    "        score.append(max_score)\n",
    "        comp.append(comparisons)\n",
    "    return [predicted_nb, idx_to_dm, score, comp, empty_idx_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percorsi all'elenco dei brand elettronici e dei siti di eCommerce\n",
    "brands_path = './utils/electronic_brands.txt'\n",
    "sites_path = './utils/sites.txt'\n",
    "\n",
    "# percorso al dataset di input\n",
    "dataset_path = './data/dataset_monitor.csv'\n",
    "\n",
    "# percorso al dataset di training per l'estrazione di trigrammi notevoli\n",
    "product_recognition_training_set = './data/product_recognition_train.csv'\n",
    "\n",
    "# precorso al modello allenato di DeepMatcher per la classificazione del matching\n",
    "deepMatcher_model_path = './data/deepMatcher_model.pth'\n",
    "\n",
    "# sample del dataset da valutare\n",
    "n_sample = 30000\n",
    "\n",
    "# soglia similarità [0, 1]\n",
    "threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(brands_path, 'r')\n",
    "brands = f.read().splitlines()\n",
    "f.close()\n",
    "f = open(sites_path, 'r')\n",
    "sites = f.read().splitlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe iniziale\n",
    "df = pd.read_csv(dataset_path)\n",
    "columns = ['left_spec_id', 'right_spec_id', 'left_page_title', 'right_page_title', 'left_brand', 'right_brand', 'label']\n",
    "df = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codice\n",
    "regex_code = re.compile('^(?=.*[a-zA-Z])(?=.*[0-9])')\n",
    "\n",
    "#caratteristiche tecniche\n",
    "regex_px = re.compile('\\d+[xX]\\d+$')\n",
    "regex_mem = re.compile('\\d+(MB|GB|TB|mb|gb|tb|Mb|Gb|Tb|mB|gB|tB){1}$')\n",
    "regex_eia = re.compile('RS\\d+$')\n",
    "regex_proc = re.compile('(i\\d$|i\\d-)')\n",
    "regex_inch = re.compile('(inch|Inch|INCH|in|IN|In)')\n",
    "regex_freq = re.compile('(Ghz|GHZ|ghz|GHz|Hz|HZ|hz)')\n",
    "regex_ordinal = re.compile('(1st|2nd|3rd|\\dth$)')\n",
    "regex_bit = re.compile('(\\d+-bit|\\d+Bit|\\d+BIT|\\d+-Bit|\\d+-BIT)')\n",
    "regex_hasNum = re.compile('\\d')\n",
    "\n",
    "# lista delle regex sulle caratteristiche tecniche\n",
    "specs_re = [regex_px, regex_mem, regex_eia, regex_freq, regex_inch, regex_mem, regex_ordinal, regex_proc, regex_px]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SVM e MLP per Product Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(product_recognition_training_set, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for index, row in df_train.iterrows():\n",
    "    d = {}\n",
    "    vec = (json.loads(row['vector']))\n",
    "    for i in range(0, 12):\n",
    "        d['feature_' + str(i)] = vec[i]\n",
    "    d['label'] = row['code']\n",
    "    l.append(d)\n",
    "\n",
    "#df usato per allenare classificatori\n",
    "df_features = pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features.drop('label', 1, inplace=False), df_features['label'], test_size=0.1,random_state=109) # 90% training and 10% test\n",
    "\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "not_code = X[X.label==0]\n",
    "code = X[X.label==1]\n",
    "code_upsampled = resample(code,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(not_code), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "upsampled = pd.concat([not_code, code_upsampled])\n",
    "\n",
    "X_train = upsampled.drop('label', axis=1)\n",
    "y_train = upsampled.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'C': 10, 'gamma': 1}\n",
      "GridSearchCV(cv=10, estimator=SVC(),\n",
      "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10],\n",
      "                         'gamma': [0.001, 0.01, 0.1, 1]})\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Trainining SVM con ottimizzazione iperparametri\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "clf_svm = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=10)\n",
    "clf_svm.fit(X_train, y_train)\n",
    "print('Best parameters found:\\n', clf_svm.best_params_)\n",
    "print(clf_svm)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_svm = clf_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aparo/jupyter/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Trainining MLP con ottimizzazione iperparametri\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "clf_mlp = GridSearchCV(MLPClassifier(max_iter=100), parameter_space, n_jobs=-1, cv=3)\n",
    "clf_mlp.fit(X_train, y_train)\n",
    "print('Best parameters found:\\n', clf_mlp.best_params_)\n",
    "\n",
    "y_pred_mlp = clf_mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversione in trigrammi di tutti i titoli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_titles = list(df['left_page_title'])\n",
    "left_titles_tokenized = []\n",
    "for item in left_titles:\n",
    "    no_punc = (item.replace('\"', '').replace(\"Null\", \"\").translate(str.maketrans('', '', string.punctuation)))\n",
    "    tokenized = ngrams(no_punc.split(), 3)\n",
    "    left_titles_tokenized.append([' '.join(grams) for grams in tokenized])\n",
    "\n",
    "right_titles = list(df['right_page_title'])\n",
    "right_titles_tokenized = []\n",
    "for item in right_titles:\n",
    "    no_punc = (item.replace('\"', '').replace(\"Null\", \"\").translate(str.maketrans('', '', string.punctuation)))\n",
    "    tokenized = ngrams(no_punc.split(), 3)\n",
    "    right_titles_tokenized.append([' '.join(grams) for grams in tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999/1000\r"
     ]
    }
   ],
   "source": [
    "#df_final_left = pd.DataFrame(left[0])\n",
    "#df_final_right = pd.DataFrame(right[0])\n",
    "final_list_left = prepare_final_data(df, left_titles_tokenized,'left', clf_svm)\n",
    "final_list_right = prepare_final_data(df, right_titles_tokenized,'right', clf_svm)\n",
    "df_final_left = pd.DataFrame(final_list_left)\n",
    "df_final_right = pd.DataFrame(final_list_right)\n",
    "\n",
    "df_final = pd.concat([df_final_left,df_final_right],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample randomico del df in input\n",
    "def sample_df(df, n_sample):\n",
    "    if n_sample >= len(df):\n",
    "        return df\n",
    "    return df.sample(n=n_sample).reset_index()\n",
    "\n",
    "# creazione della lista in input alla funzione che calcola la similarità tra due offerte\n",
    "def extract_columns(df):\n",
    "    return [df['index'].tolist(), df['left_spec_id'].tolist(), df['left_codes'].tolist(), df['left_brand'].tolist(), df['right_spec_id'].tolist(), df['right_codes'].tolist(), df['right_brand'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_eval = sample_df(df_final, n_sample)\n",
    "eval_input = extract_columns(df_to_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valutazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\r"
     ]
    }
   ],
   "source": [
    "code_based_output = compute_score(eval_input, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo dei confronti tra offerte valutati con deepMatcher\n",
    "def compute_combined(df, cb_output, model_path):\n",
    "    try:\n",
    "        vocab._default_unk_index\n",
    "    except AttributeError:\n",
    "        def _default_unk_index():\n",
    "            return 0\n",
    "        vocab._default_unk_index = _default_unk_index\n",
    "    \n",
    "    output = cb_output\n",
    "    \n",
    "    index_list = output[1]\n",
    "    to_dm = df.iloc[index_list]\n",
    "    to_dm['id'] = [num for num in range(0, len(to_dm))]\n",
    "    del to_dm['label']\n",
    "    to_dm.to_csv('./unlabeled_to_dm.csv', index=False)\n",
    "    model = dm.MatchingModel()\n",
    "    model.load_state(model_path)\n",
    "    unlabeled_proc = dm.data.process_unlabeled(path='./unlabeled_to_dm.csv', trained_model=model)\n",
    "    predictions = model.run_prediction(unlabeled_proc, output_attributes=True)\n",
    "    pred_output = [0 if score < 0.9 else 1 for score in predictions['match_score']]\n",
    "    dm_df = pd.DataFrame(data=pred_output, columns=[\"label\"])\n",
    "    \n",
    "    labels = dm_df['label']\n",
    "    for i, idx in enumerate(output[4]):\n",
    "        output[0][idx] = labels[i]\n",
    "    \n",
    "    return output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-4c5b7b638b2a>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  to_dm['id'] = [num for num in range(0, len(to_dm))]\n",
      "/Users/aparo/jupyter/lib/python3.9/site-packages/torchtext/data/field.py:150: UserWarning: MatchingField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "\n",
      "Reading and processing data from \"./unlabeled_to_dm.csv\"\n",
      "/Users/aparo/jupyter/lib/python3.9/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/Users/aparo/jupyter/lib/python3.9/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "0% [############################# ] 100% | ETA: 00:00:00Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>  PREDICT Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aparo/jupyter/lib/python3.9/site-packages/torchtext/data/iterator.py:48: UserWarning: MatchingIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/aparo/jupyter/lib/python3.9/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "0% [██] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:    5.4 | Load Time:    0.1 || F1:   0.00 | Prec:   0.00 | Rec:   0.00 || Ex/s:   0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_output = compute_combined(df, code_based_output, deepMatcher_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valutazione score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.782\n",
      "f1-score: 0.877665544332211\n"
     ]
    }
   ],
   "source": [
    "y_true = df.loc[eval_input[0]]['label'].tolist()\n",
    "\n",
    "print(\"Precision:\",metrics.precision_score(y_true, combined_output[0]))\n",
    "print(\"Recall:\",metrics.recall_score(y_true, combined_output[0]))\n",
    "print(\"f1-score:\", metrics.f1_score(y_true, combined_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
